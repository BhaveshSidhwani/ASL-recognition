{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "OPMSyFL1BcNt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "7p5CvmBwByD5"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('./sign_mnist_train.csv')\n",
        "test = pd.read_csv('./sign_mnist_test.csv')\n",
        "combined_dataset = pd.concat([train, test], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLo812KKC9Hi",
        "outputId": "aa364b1f-7448-4893-ebef-512930b608ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
            "0      3     107     118     127     134     139     143     146     150   \n",
            "1      6     155     157     156     156     156     157     156     158   \n",
            "2      2     187     188     188     187     187     186     187     188   \n",
            "3      2     211     211     212     212     211     210     211     210   \n",
            "4     13     164     167     170     172     176     179     180     184   \n",
            "\n",
            "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
            "0     153  ...       207       207       207       207       206       206   \n",
            "1     158  ...        69       149       128        87        94       163   \n",
            "2     187  ...       202       201       200       199       198       199   \n",
            "3     210  ...       235       234       233       231       230       226   \n",
            "4     185  ...        92       105       105       108       133       163   \n",
            "\n",
            "   pixel781  pixel782  pixel783  pixel784  \n",
            "0       206       204       203       202  \n",
            "1       175       103       135       149  \n",
            "2       198       195       194       195  \n",
            "3       225       222       229       163  \n",
            "4       157       163       164       179  \n",
            "\n",
            "[5 rows x 785 columns]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(34627, 785)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(combined_dataset.head())\n",
        "combined_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes:  [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
          ]
        }
      ],
      "source": [
        "labels = combined_dataset['label'].values\n",
        "unique_val = np.array(labels)\n",
        "print(\"Classes: \",np.unique(unique_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_dataset.drop('label', axis = 1, inplace = True)\n",
        "images = combined_dataset.values\n",
        "images = np.array([np.reshape(i, (28, 28)) for i in images])\n",
        "images = np.array([i.flatten() for i in images])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "# label_binrizer = LabelBinarizer()\n",
        "# labels = label_binrizer.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=13)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=13)\n",
        "\n",
        "## out of total data, 70% is for train, 10% for validation and 20% for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1718021088.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[104], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    train=[for (torch.tensor(X_train[i]),y_train[i]) in for i in range(len(X_train))]\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# for (torch.tensor(X_train[i]),y_train[i]) in\n",
        "# train=\n",
        "# for i in range(len(X_train)):\n",
        "\n",
        "# print(train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(images[10].reshape(28,28))\n",
        "print(labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  \n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long) \n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Initialize DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "# Initialize DataLoader\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "784\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i,data in enumerate(train_loader):\n",
        "    # print(data)\n",
        "    break\n",
        "print(len(train_dataset[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_accuracy_part34(loader, model):\n",
        "  if loader.dataset.train:\n",
        "    print('Checking accuracy on validation set')\n",
        "  else:\n",
        "    print('Checking accuracy on test set')\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()  # set model to evaluation mode\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n",
        "      y = y.to(device=device, dtype=torch.long)\n",
        "      scores = model(x)\n",
        "      _, preds = scores.max(1)\n",
        "      num_correct += (preds == y).sum()\n",
        "      num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "def train_part34(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
        "\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Accuracy, also prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    acc = 0\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=torch.float)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % 100 == 0:\n",
        "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
        "                acc = check_accuracy_part34(val_loader, model)\n",
        "                print()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 784]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[120], line 92\u001b[0m\n\u001b[1;32m     83\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# You should get at least 70% accuracy\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtrain_part34\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[118], line 25\u001b[0m, in \u001b[0;36mtrain_part34\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)  \u001b[38;5;66;03m# move to device, e.g. GPU\u001b[39;00m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 25\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(scores, y)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Zero out all of the gradients for the variables which the optimizer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# will update.\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[120], line 68\u001b[0m, in \u001b[0;36mmodel_design.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 68\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(output)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# output = F.relu(output)\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 784]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
        "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
        "#                                                                              #\n",
        "# Note that you can use the check_accuracy function to evaluate on either      #\n",
        "# the test set or the validation set, by passing either loader_test or         #\n",
        "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
        "# the test set until you have finished your architecture and  hyperparameter   #\n",
        "# tuning, and only run the test set once at the end to report a final value.   #\n",
        "################################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "\n",
        "class Single(nn.Module):\n",
        "    def __init__(self, input_p, planes):\n",
        "        super(Single, self).__init__()\n",
        "        #convolution and batchNorm 1\n",
        "        self.conv_1 = nn.Conv2d(input_p, planes, 3, padding = 1)\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        #convolution and batchNorm2\n",
        "        self.conv_2 = nn.Conv2d(planes, planes, 3, padding = 1)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.short = nn.Sequential()\n",
        "        if (input_p != planes):\n",
        "            self.short = nn.Sequential(nn.Conv2d(input_p, planes, 3, padding = 1), nn.BatchNorm2d(planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv_1(x)\n",
        "        output = self.batch_norm_1(output)\n",
        "        # output = F.relu(output)\n",
        "        output = F.leaky_relu(output)\n",
        "        output = self.conv_2(output)\n",
        "        output = self.batch_norm_2(output)\n",
        "        output = output + self.short(x)\n",
        "        # output = F.relu(output)\n",
        "        output = F.leaky_relu(output)\n",
        "        return output\n",
        "\n",
        "class model_design(nn.Module):\n",
        "    def __init__(self, input_l, hidden_l, no_of_class):\n",
        "        super(model_design, self).__init__()\n",
        "\n",
        "        #convolution and batchNorm\n",
        "        self.conv = nn.Conv2d(input_l, hidden_l[0], 3, padding = 1)\n",
        "        self.batch_norm = nn.BatchNorm2d(hidden_l[0])\n",
        "\n",
        "        self.resnet_1 = Single(hidden_l[0], hidden_l[1])\n",
        "        self.resnet_2 = Single(hidden_l[1], hidden_l[2])\n",
        "\n",
        "        #pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(hidden_l[2] * 16 * 16 , no_of_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv(x)\n",
        "        output = self.batch_norm(output)\n",
        "        # output = F.relu(output)\n",
        "        output = F.leaky_relu(output)\n",
        "\n",
        "        output = self.resnet_1(output)\n",
        "        output = self.resnet_2(output)\n",
        "        output = self.pool(output)\n",
        "        output = self.fc(flatten(output))\n",
        "        return output\n",
        "\n",
        "hidden_layers=[16, 32, 64]\n",
        "model=model_design(3, hidden_layers, 10)\n",
        "# model=model.to('cuda')\n",
        "# optimizer = optim.Adagrad(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "\n",
        "\n",
        "# END OF YOUR CODE\n",
        "\n",
        "# You should get at least 70% accuracy\n",
        "train_part34(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
